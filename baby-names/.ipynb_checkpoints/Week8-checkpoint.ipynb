{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Stream Structured Data\n",
    "\n",
    "*With some initial hickups, local Spark installation has been used.*\n",
    "\n",
    "In the first part of the exercise, you will create a simple Spark streaming program that reads an input stream from a file source. The file source stream reader reads data from a directory on a file system. When a new file is added to the folder, Spark adds that fileâ€™s data to the input data stream.\n",
    "\n",
    "You can find the input data for this exercise in the baby-names/streaming directory. This directory contains the baby names CSV file randomized and split into 98 individual files. You will use these files to simulate incoming streaming data.\n",
    "\n",
    "### a. Count the Number of Females\n",
    "\n",
    "In the first part of the exercise, you will create a Spark program that monitors an incoming directory. To simulate streaming data, you will copy CSV files from the baby-names/streaming directory into the incoming directory. Since you will be loading CSV data, you will need to define a schema before you initialize the streaming dataframe.\n",
    "\n",
    "From this input data stream, you will create a simple output data stream that counts the number of females and writes it to the console. Approximately every 10 seconds or so, copy a new file into the directory and report the console output. Do this for the first ten files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import libraries\n",
    "import os.path \n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "import os.path \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from time import sleep\n",
    "from pyspark.sql.functions import window\n",
    "\n",
    "# create sprk context\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directories\n",
    "file_path = r'/home/dsc650-master/data/baby-names/streaming'\n",
    "ipstreaming_file_path = r'/home/dsc650-master/input_streaming'\n",
    "batchstream_file_path = r'/home/dsc650-master/batch_streaming'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the schema from the statis file\n",
    "spark = SparkSession.builder.appName('strstream').getOrCreate()\n",
    "static = spark.read.csv(file_path, header = True)\n",
    "dataschema = static.schema\n",
    "\n",
    "# check structure\n",
    "static.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading stream from input directory\n",
    "streaming = spark.readStream.schema(dataschema).csv(ipstreaming_file_path) \n",
    "\n",
    "# check count\n",
    "counts = streaming.groupBy(\"sex\").count()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop stream\n",
    "streamingquery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingquery = counts.writeStream.queryName(\"counts\").format(\"memory\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print active streams\n",
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(static.isStreaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display stream progress\n",
    "for x in range(10):\n",
    "    spark.sql(\"SELECT * FROM counts\").show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileslist = os.listdir(file_path)\n",
    "print(fileslist[1:11])\n",
    "\n",
    "# getting list of files in the directory\n",
    "files_list = glob.glob(\"/home/dsc650-master/data/baby-names/streaming/*.csv\")\n",
    "print(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display sourec and destination path\n",
    "print(os.path.basename(file_path))\n",
    "print(os.path.dirname(files_list[1]))\n",
    "print(os.path.split(files_list[1]) )\n",
    "\n",
    "filename = os.path.splitext(files_list[1])[0]\n",
    "print(filename)\n",
    "\n",
    "print(os.path.join('input_streaming', os.path.dirname(files_list[1])))\n",
    "\n",
    "src_path = os.path.join(file_path,fileslist[4])\n",
    "\n",
    "dest_path = os.path.join(ipstreaming_file_path,fileslist[4])\n",
    "\n",
    "print(src_path, dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display counts\n",
    "for i in range(len(fileslist[1:11])):\n",
    "    \n",
    "    file = fileslist[i]\n",
    "    \n",
    "    print(file)\n",
    "    src_path = os.path.join(file_path,file)\n",
    "    dest_path = os.path.join(ipstreaming_file_path,file)\n",
    "    \n",
    "    shutil.copy(src_path, dest_path) \n",
    "    print(\"File moved \\n\")\n",
    "    \n",
    "    \n",
    "    print(\"Check counts \\n \")\n",
    "    sleep(2)\n",
    "    spark.sql(\"SELECT * FROM counts\").show()\n",
    "    sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Micro-Batching\n",
    "\n",
    "Repeat the last step, but use a micro-batch interval to trigger the processing every 30 seconds. Approximately every 10 seconds or so, copy a new file into the directory and report the console output. Do this for the first ten files. How did the output differ from the previous example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# define streaming context\n",
    "stc = Streamingcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform and read stream from input directory\n",
    "csvmb = spark.readStream.schema(dataschema).csv(batchstream_file_path)  \n",
    "batch_counts = csvmb.groupBy(\"sex\").count()\n",
    "\n",
    "# get count of females\n",
    "batch_counts.select(\"sex\").where(\"sex = 'F'\")\n",
    "batch_counts.groupby(\"sex\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define bacth writer\n",
    "microbatch_writer = batch_counts.\\\n",
    "  writeStream.\\\n",
    "  trigger(processingTime = '30 seconds').\\\n",
    "  queryName(\"batch_counts\").\\\n",
    "  format(\"memory\").\\\n",
    "  outputMode(\"complete\").\n",
    "  start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start batch writer\n",
    "microbatch_writer.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display micro batch result\n",
    "for i in range(len(fileslist[1:11])):\n",
    "    \n",
    "    file = fileslist[i]\n",
    "    \n",
    "    print(file)\n",
    "    src_path = os.path.join(file_path,file)\n",
    "    dest_path = os.path.join(batchstream_file_path,file)\n",
    "    \n",
    "    shutil.copy(src_path, dest_path) \n",
    "    print(\"File moved \\n\")\n",
    "    \n",
    "    \n",
    "    print(\"Check the counts \\n \")\n",
    "    sleep(1)\n",
    "    spark.sql(\"SELECT * FROM batch_counts\").show()\n",
    "    sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "Copy of Week8-Copy1",
  "notebookId": 3917026457204149
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
